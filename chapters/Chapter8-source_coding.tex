\usechapterimagetrue
\chapterimage{zip.png} 
\chapter{Data compression}
\usechapterimagefalse
%------------------------------------------------
In the previous chapter we posed a series of conditions that information measures should possess. We built on top of those conditions and found a series of information measures satisfying them. 

In this chapter we will begin a journey to show that not only entropy is a good measure for information according to our desired properties, but also that it carries a strong operational meaning. In fact, we will show that matching  our intuition, if an ensemble has a certain entropy, then the length of a message that can communicate the content of the ensemble can not be smaller the entropy of then ensemble. 

%We will also begin to investigate the difference between proofs of existence and constructive methods. This will be a common pattern in information theory, though of limited importance in our introductory course. In fact, it is in many circumstances, possible to prove that codes with desired properties must exist by relying on random ensembles of codes, while actually pointing an example is more complicated.

%something about zip files

\booksection{Transmitting the weather forecast}

Let us first go back to our example from the previous chapter. It will give us enough material to formalize the discussion. Let us suppose again that we have a distant but committed friend that has agreed to send us a daily message summarizing the weather forecast. However, we want a little bit more information that in the previous chapter and now we want our friend to tell us whether tomorrow will be sunny, will snow, will rain or will hail. Let us investigate different ways in which our friend can communicate this information to us:

A first simple way would be to send a message containing the appropriate word: "sun", "rain", "snow", "hail". However, since there are only four possible messages, we could instead just send a number: $0$ for rain, $1$ for sun, $2$ for snow and $3$ for hail. 

\begin{definition}
A symbol code is a map $C:\mathcal A\mapsto\mathcal C^*$. We associate every symbol $a$ from the alphabet $\mathcal A$ with $C(a)$ a sequence of symbols from the code alphabet $\mathcal C$. We call $C(a)$ the codeword of $a$ and denote by $|C(a)|$ its length. %length notation elsewhere?
% define somewhere ^*
\end{definition}

We could be more sophisticated. Let us assume that the four events occur with the following probability: it rains with probability $1/2$, it snows with probability $1/4$, and both sunny days and hailstone days happen with probability $1/8$. We could assign the following codewords to each event: rain $0$, snow $10$, sun $110$ and hail $111$. That is, we have assigned more likely events shorter codewords? Why is this better?

\begin{definition}
we define the average length of the code as follows:
\begin{equation}
l(C)=\sum_{x\in\mathcal X}p(x)|C(x)|
\end{equation}
where $|C(x)|$ is the length of the codeword of event $x$.
\end{definition}
\begin{example}
has average length 2 while the second code with variable length has average length 1.375 which is significantly shorter. 
\end{example}

Can we do better than 1.375? We can indeed, consider the code that assigns the events rain and sun the codeword 0 and the events snow and hail the codeword 1. 
This code has average length 1. 
However, what is the problem? 
The problem is that if we receive the message 0, we can not know whether tomorrow is going to rain or to be sunny. 
The code is not-singular.

\begin{definition}
A code $C:\mathcal X\mapsto\mathcal Y$ is singular if $\forall x,y\in\mathcal X$ with $x\neq y$ $C(x)\neq C(y)$.
\end{definition}

However, in order to recover the information content, it is not enough for a code to be singular. Consider the following code: rain 0, sun 1, hail $01$ and snow $10$. 
And suppose that our friend wants to send the forecast of two consecutive days. 
If we receive the message $010$, we could decode it as $01$ and $0$ with the meaning first day hail second day rain or we could decode it as $0$ and $10$ with the meaning rain and snow.
The code can not be uniquely decoded.

\begin{definition}
Uniquely decodable
\end{definition}

In the example, it was fairly easy to spot that the code was not uniquely decodable. However, for some codes it is not so simple.
\begin{exercise}
Let $C=\{asdfsadf\}$ is $C$ uniquely decodable? (Hint: if you can not find the solution, continue reading and return to this exercise once you have understood the Sardinas-Patterson method)
\end{exercise}

Let us now investigate a method to find if a code is uniquely decodable or not. This method was proposed by Sardinas and Patterson \cite{} and is also known as the method of the dangling suffixes. 

\booksection{Length limits for lossless codes}
Once presented an information measure, we review some of its operational interpretations, in particular its relation with the theoretical limits for data compression.

A source code $C$ in alphabet $\mathcal{Y}$ for a random variable taking values in $\mathcal{X}$ is a function $c:\mathcal{X}\longrightarrow \mathcal{Y}^*$, where $\mathcal{Y}^*$ is any finite direct product of $\mathcal{Y}$. We call $c(x)$ the codeword for $x$ and $l(x)=|c(x)|$ the length of the codeword for $x$.

The length of a source code $C$ is the sum of the lengths of every codeword in $C$ weighted by its relative frequency.
\begin{equation}
L(C(X)) = \sum_{x} p(x) l(x)
\end{equation}

A source code is uniquely decodable if any concatenation of codewords can only be generated by a unique concatenation of source symbols. In other words, the source symbols generating the codewords can be recovered with no possible equivocation. 

The next inequality on the length of uniquely decodable source codes was first proved by McMillan~\cite{McMillan_56} however the following proof is a simpler version by Karush~\cite{Karush_61,Cover_91}.

\begin{theorem} The length of a uniquely decodable source code $C$ for a random variable ${X}$ taking values in alphabet $\mathcal{Y}$ verifies:
\begin{equation*}
\sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \leq 1
\end{equation*}
\label{th:mcmillan}
\end{theorem}
\begin{proof}
Let $c(x_1, x_2, ..., x_k)$ be a concatenation of codewords of aggregated length $l(x_1, x_2, ..., x_k)=\sum_{i=1}^k l(x_i)$. Since $C$ is uniquely decodable for any aggregated length $k$, no more than $|\mathcal{Y}|^k$ different concatenation of codewords can be generated. 

We can consider the related expression on the aggregated length:
\begin{eqnarray}
\left( \sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{x_1}\frac{1}{|\mathcal{Y}|^{l(x_1)}} \sum_{x_2}\frac{1}{|\mathcal{Y}|^{l(x_2)}} \dots \sum_{x_n }\frac{1}{|\mathcal{Y}|^{l(x_n)}} \nonumber \\
&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1)+l(x_2)+\dots +l(x_n)}} \nonumber \\
&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1+x_2+\dots +x_n)}}
\end{eqnarray}
\noindent which can also be written as the sum for all possible lengths $i$ of the number $T_i$ of concatenation of $n$ codewords:

\begin{eqnarray}
\left( \sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{i=1}^{nl_{max}}\frac{T_i}{|\mathcal{Y}|^i} \nonumber \\
&\leq & \sum_{i=1}^{nl_{max}}\frac{|\mathcal{Y}|^i}{|\mathcal{Y}|^i} \nonumber \\
&\leq & nl_{max}
\end{eqnarray}
\noindent where $l_{max} = \max_{x}l(x)$. And taking the $n$-th root in both sides:

\begin{eqnarray}
\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq & (nl_{max})^{1/n}
\end{eqnarray}
\noindent now since the limit $\lim_{n \to \infty } (nl_{max})^{1/n} = 1$ and the result holds for all $n$:

\begin{eqnarray}
\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq 1
\end{eqnarray}
\end{proof}

We finish this brief overview of source coding with Th.~\ref{th:sourcecoding}~\cite{Cover_91}, it shows that the length  of a uniquely decodable code is lower bounded by the entropy of the random variable.
\begin{theorem}
\label{th:sourcecoding}
The length of a uniquely decodable code taking values from finite alphabet $\mathcal{Y}$ for random variable ${X}$ is lower bounded by the entropy of ${X}$.
\begin{equation*}
L\geq H_{|\mathcal{Y}|}({X})
\end{equation*}
\end{theorem}
\begin{proof}
\begin{eqnarray}
H_{|\mathcal{Y}|}({X}) - L &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x}p(x)l(x) \nonumber \\
                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x} p(x)  \log_{|\mathcal{Y}|} |\mathcal{Y}|^{-l(x)} \nonumber \\
                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{|\mathcal{Y}|^{-l(x)} }{p(x)} \nonumber \\
                                      &\leq & \log_{|\mathcal{Y}|} \sum_{x} |\mathcal{Y}|^{-l(x)} \nonumber \\
                                      &\leq & \log_{|\mathcal{Y}|} 1 = 0
\end{eqnarray}
\noindent where the first inequality is again an application of Jensen's result Th.~\ref{th:jensen} and the second one results from applying McMillan's Th.~\ref{th:mcmillan}.
\end{proof}
This result can be extended to consider a source code $C$ for a sequence of $n$ random variables {iid} from the ensemble ${X}$. That is, a source  ${X}$ as we defined it in Sec.~\ref{subsec:entropy}. Let $R=L(C({X^n}))/n$, the length per symbol of $C$, be the encoding rate of the source ${X}$. It is trivial to see that the rate is also lower bounded by the entropy of the source: \begin{eqnarray} R &=& L(C({X^n}))/n \nonumber\\
  &\geq& H_{|\mathcal{Y}|}({X^n})/n \nonumber\\
 &=& H_{|\mathcal{Y}|}({X})
\end{eqnarray}

\booksection{Huffman codes and their optimality}
We will now investigate a scheme that achieves the optimal encoding rate of a source. The algorithm is remarkably simple and was discovered by Huffman \cite{} while a master student. Let us describe the algorithm.

Given an ensemble $X$ and let $C$ denote the Huffman code of $X$.
\begin{enumerate}
\item Let $a,b$ be two symbols with smallest probability. Create ensemble $X'$ identical to $X$ but replacing $a,b$ with a new symbol $c$ that has probability $p_{X'}(c)=p_X(a)+p_X(b)$. 
\item Let $C(a)=C(c)0$ and $C(b)=C(c)1$.
\item If $X'$ has an alphabet with more than one symbol go back to 1.
\end{enumerate}
A first observation on the algorithm is that computationally it is very simple. Since at each iteration there is one symbol less it will terminate in $\mathcal A_X$ iterations.
\begin{exercise}
Find the Huffman code of an ensemble $X$ with symbols $\mathcal A_X=\{x_1,x_2,x_3,x_4,x_5,x_6\}$ and corresponding probabilities $p_X=\{0.3,0.2,0.18,0.15,0.12,0.05\}$.
\end{exercise}
\begin{solution}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{ccc} 
      0.3             &0.30&0\\
      0.2             &0.20&0\\
      0.18            &0.18                        &0
      %0.15            &\tikzmark{c}0.17\tm{e}&0\\
      %0.12\tikzmark{a}&            0.15\tm{d} \\
      %0.05\tikzmark{b}&   &  \\
    \end{tabular}
  \end{center}
\end{table}
  \begin{tikzpicture}[overlay, remember picture, yshift=.10\baselineskip, shorten >=.5pt, shorten <=.5pt]
    %\draw [->] ({pic cs:a}) [bend left] to ({pic cs:b});
    \draw [-] ([yshift=.0pt]{pic cs:a}) -- ({pic cs:c});
    \draw [-] ([yshift=.0pt]{pic cs:b}) -- ({pic cs:c});
  \end{tikzpicture}
\end{solution}
\booksection{Arithmetic codes}
\booksection{Exercises}
\booksection{Further reading}
Both Shannon's original paper \cite{Shannon_48} and Cover and Thomas \cite{Cover_91} (chapter 5) provide further detail into the topics discussed here. A popular introduction to source coding is provided by xkcd (the comic stripe) \cite{xkcd}, in the post you will recognize the image that opens the chapter, perhaps you can explain it's relation with the topic. %https://what-if.xkcd.com/34/

An important remark on our previous discussion is that we have only covered lossless codes. That is codes where no information is lost. While we showed that it is not possible to reliably code at rates below entropy, in many practical applications a certain amount of loss or unreliability is acceptable if in exchange one can increase the compression efficiency. 

and the story goes that while a master student, a professor told the students that if they wrote a report on the optimal code for a source, they could skip the exam. Of course, the professor did not 

Data compression has direct application in our daily activities: our emails are compressed using lossless compression algorithms
