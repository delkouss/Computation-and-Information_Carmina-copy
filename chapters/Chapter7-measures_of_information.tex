\chapterimage{chapter_head_shannon.jpg} % Chapter heading image
\chapter{Measures of information}
%------------------------------------------------
This is the age of information.
We open documents, consume media, exchange text messages, perform videoconferences or watch the weather forecast to name a few examples.
But what is exactly information? Can we quantify it? Is it possible to say that
one weather forecast contains more information than another? In this chapter,
we will learn that this is indeed possible. Our exposition, follows to a great
extent the visionary paper of Claude Shannon in 1948.

\booksection{Surprising information}
Let us warm up with a series of questions. 
The premise of all of them is the following: 
For some reason you want to know the weather forecast for tomorrow, but don't want to watch it or read it yourself. 
However, in order to get some information that allows you to choose appropriately your clothing for the next day, you ask a friend to send you a message to your phone every night with a summary of the forecast. 
You agree on a very simple encoding for the message, your friend will send a $1$ if the forecast predicts precipitations and $0$ otherwise. Think about these questions and come back to them after reading the whole chapter.
\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{l|l|l} 
      \textbf{Location} & \textbf{Days of rain} & \textbf{Days with no rain}\\
      \hline
      \text{Rotterdam\footnote{source:knmi.nl}} & 153 & 212\\%\footnote{source:knmi.nl}
      \text{Atacama desert} & 5 & 360
    \end{tabular}
    \caption{Summary of precipitations in the year 2018.}
  \end{center}
\end{table}
\begin{exercise}
Let us assume that you are living in the Atacama desert where it rarely rains and you receive a $0$. How much information does this message carry? 
\end{exercise}
\begin{exercise}
Now let us assume that you live in the Netherlands where it does rain quite often, but certainly not every day. You also receive a $0$, does the message contain information? What about if you receive a $1$? Does the $1$ message contain more or less information than the message $0$? 
\end{exercise}
\begin{exercise}
\label{ex:infcontext}
Finally, let us assume that you live in the Netherlands and you happen to know that it is mid August. Does the message $0$ carry the same information as in winter?
\end{exercise}

Before you continue reading, pause for a moment and think what is common in your answers.

We have posed these questions to suggest a relation between the amount of information a message provides and how surprising it is. 
We will make this connection stronger in the rest of these chapter. %Now, what other properties do you think an information measure should satisfy?
%Information is never negative, i.e. if we know already the content of a message, we can say that it contains zero information. 
%This is the worst case, in any other case, the content of the message will always complement our understanding of the world and will in consequence contain some information. 
%Finally, the information we learn, when we learn about unrelated events should add up. 
%The sum of the informatio nthat we elarn when we know that tomorrow will rain and the one we learn when we know that we have won the lottery should be the same as the information we learn when we learn both events. However, as we saw in the weather information, context does play a role. Learning that it will rain, will have different value depending on the location or on the time of the year. In the following we will make use of our desided properties of an information quantity and find a mathematical function that fulfills them all.

\booksection{Refresher on probability theory}
A basic understanding of probability theory is essential for the material that follows. 
Let us review the fundamental concepts and definitions together with the notation that we will use here. 
As we will only deal with discrete probability distributions, the definitions that follow are not fully general but sufficient for our purposes.
If you have troubles following this section and doing the exercises please go back to your undergraduate text on the topic. 

Given a finite set $\mathcal X$, we call a probability distribution a function $p:\mathcal X\rightarrow[0,1]$, that is a function from the elements of $\mathcal X$ to the closed interval in the real line between zero and one, with the condition that $\sum_{x\in\mathcal X}p(x)=1$. 
Note that it follows automatically from our definition that for all $x\in\mathcal X$ $p(x)\geq 0$. 
\begin{example} Let $\mathcal X =\left\{\text{tails},\text{heads}\right\}$ we could define a probability distribution function $p$ such that $p(\text{heads})=0.3$ and $p(\text{tails})=0.7$.
\end{example}
\begin{example} An important example is the uniform distribution. Given a finite set $\mathcal X$, a uniform distribution on the set $\mathcal X$ is a function $p$ that for all $x\in\mathcal X$ assigns the value $$p(x)=\frac{1}{|\mathcal{X}|}\ .$$ where, we denote by $|\cdot|$ the number of elements in the set.
\end{example}
We define an ensemble $X$ as the tuple of a probability distribution $p_X$ together with its domain $\mathcal A_X$. 
Generally, we will refer to $\mathcal A_X$ as the sample space of $X$ and to its elements as events. %and its elements differently depending on the context. as the sample space and to any non empty subset of it as an event. 
In information theory, $X$ typically models an object in a communications setup (see \ref{}), in this context it is common to call $\mathcal A_X$ the alphabet of $X$ and refer to its elements as letters. %and its elements differently depending on the context. as the sample space and to any non empty subset of it as an event. 

Note that we can extend the definition of $p_X$ to any subset $\mathcal{S}\subseteq \mathcal{A}_X$:
\begin{equation}
p_X(\mathcal{S})=\sum_{x\in \mathcal{S}}p_X(x)
\end{equation} 
\begin{example}
Let $X$ be an ensemble with alphabet $\mathcal A_X=\{1,2,3\}$ and with $p_X$ the uniform distribution. Then if $\mathcal S=\{1,2\}$, $p(\mathcal S)=1/3+1/3=2/3$ 
\end{example}
Abusing notation, we will also call event any subset of the alphabet of an ensemble. For this particular case of set we will drop the caligraphic notation for sets.
Let $a$ and $b$ be two events in $\mathcal{X}$, we define $a\cup b$ and $a\cap b$ as the union and intersection of $a$ and $b$. 
$a\cup b$ is the event that contains all outcomes belonging to $a$, to $b$ and to both, we will also denote the event $a\cup b$ by $a \text{ or } b$. 
$a\cap b$ is the event that contains all outcomes belonging to both $a$ and $b$, we will also denote the event $a\cap b$ by $a \text{ and } b$. Two events are disjoint if their intersection is null.

Given an ensemble $X$ and two events $a,b$ we say that they are independent if:
\begin{equation}
p_X(a \text{ and } b)=p_X(a)p_X(b)
\end{equation}
Let $a$ and $b$ be two events with non zero probability. We call $p_X(a|b)=p_X(a \text{ and } b)/p_X(b)$ the conditional probability of $a$ given that $b$ occurs. It follows that if and only if $a$ and $b$ are independent $p_X(a|b)=p_X(a)$.

In the following, we will use the explicit notation $p_X,\mathcal A_X$ for the probability distribution of ensemble $X$ and its alphabet whenever confusion can arise but we will drop the subscript whenever possible.
\begin{exercise}
Let $X$ be an ensemble modelling two fair coins. Identify two events $a,b$ that are independent and verify that $p_X(a|b)=p_X(a)$
\end{exercise}
\begin{solution}
A
\end{solution}
Given two alphabets $\mathcal A_X,\mathcal A_Y$ we can define a joint ensemble on them with sample space or alphabet the direct product: $\mathcal A_{XY} = \mathcal A_X \times \mathcal A_Y$. 
We can associate, as well, a probability distribution function to map all tuples $(x,y)$ to $[0,1]$. 

The probability of an event in the joint ensemble is equally defined as the sum of the probability of the individual events. 
In particular, we can define for every $x\in\mathcal X$ the probability of $p_X(x)$ as the sum of $p_{XY}(x,y)$ for all $y\in\mathcal Y$:
\begin{equation}
p_X(x)=\sum_{y}p_{XY}(x,y)
\end{equation} 
\noindent and equivalently $p_Y(y)$:
\begin{equation}
p_Y(y)=\sum_{x}p_{XY}(x,y)
\end{equation} 
\begin{example}
Consider $n$ repetitions of an experiment, each repetition can be modelled by ensemble $X$ and events in different experiments are independent. We can model the set of $n$ repetitions via the joint ensemble $X_1\ldots X_N$, where $X_i$ is the ensemble associated with the $i$-th experiment, and joint the probability distribution is given by:
\begin{equation}
p_{X_1\ldots X_N}(x_1,x_2,\ldots,x_n)=\prod_{i=1}^np_X(x_i)
\end{equation}
\end{example}
A random variable $V$ on the ensemble $X$ is a numerical function from the elements of $\mathcal A_X$ to (typically) the real line. That is, a function $V:\mathcal{A}_X\rightarrow \mathcal A_V$, where $\mathcal A_V$ is a finite subset of the reals. The random variable $V$ induces an ensemble with alphabet $\mathcal A_V$ and probability distribution $p_V$ where $p_V$ is given by:

\begin{equation}
p_V(v)=\sum_{x\in\mathcal A_X:V(x)=v}p_X(x)
\end{equation} 
for all $v\in\mathcal A_V$.

The mean or expectation of a random variable is given by:
\begin{equation}
\mathbb E[V]=\sum_{x\in\mathcal A_X}p_X(x)V(x)=\sum_{v\in\mathcal A_V}p_Vv
\end{equation}

\booksection{Axiomatic derivation of entropy}
\label{sec:axent}
Let us now try to understand what type of functions can quantify information in a satisfactory way. Let us make this investigation more precise. In particular, suppose that given some ensemble $X$ we observe the occurrence of an event $x\in\mathcal A_X$. As we informally argued in the introduction, the information we gain seems to be related to the likelihood of the event we observed. But how can we make this intuition quantitative? %What information does the observation provide?

A function that quantifies information will be a function from a subset of $\mathcal A_X$ to the reals. 
Let us call this function $h$. 
Then given some event $x$, $h(x)$ will be some number that will quantify the information we learn. 
Let us discuss what properties an ideal information quantifier should have. 

\begin{itemize}
\item The measure should be non-negative, that is, an event gives either none or some information, but it can not give negative information. That is, for all events $x\in\mathcal A_X$ we require:
         \begin{equation}
         h(x)\geq 0         
         \end{equation}
\item Suppose that we buy two lottery tickets in two different lottery games, event $x$ is: "our first ticket wins a prize", event $y$ is: "our second ticket does not win a prize". 
We expect these two events to be independent and the information content of knowing both events should be the sum of the information of the individual events.
The occurrence of two independent events should yield the same information that the occurrence of the single events would provide an observer. If we let $h$ be an information measuring function
         \begin{equation}
         p_X(x \text{ and } y)=p_X(x) p_X(y)\Rightarrow h(x\text{ and } y) = h(x)+h(y)
         \end{equation}
         %and more generally the information that $n$ independent identical events provide:
         %\begin{equation}
         %\label{eq:indepentr}
         %h(a^n) = nh(a)         
         %\end{equation}
\item Following our discussion about information and surprise, we want $h$ to quantify less probable events with a larger value than more probable events. For any two ensembles $X,Y$ and events $x\in\mathcal A_X$ and $y\in\mathcal A_Y$, we require:
         \begin{equation}
         \label{eq:incentr}
         p_X(x)< p_Y(y) \Rightarrow h(x)> h(y)
         \end{equation}
\item The final condition is that we don't want that arbitrarily small changes in probability lead to a change in the information quantity, i.e. $h$ should be a continuous function.     
\end{itemize}

It turns out that there is a very limited set of functions that verify these properties. Given some ensemble $X$, the unique family of functions is of the form: 
\begin{equation}
\label{eq:h}
         h(x) = -\log_\lambda p_X(x)
\end{equation}
\noindent where $x\in\mathcal A_X$ and with $\lambda > 1$ for the measure to be positive. 
Choosing different values of $\lambda$ allows us to measure information with different units. 

There are some common choices of $\lambda$ that give rise to well known units of information: if we let $\lambda=2$, the unit of information is called bit. When $\lambda=3$ information is measured in trits, for $\lambda=10$ the unit is called a digit and when $\lambda=e$ nat. Unless stated otherwise, in the following we will assume that $\lambda=2$ and will let $\log=\log_2$.
\begin{definition}
Given an ensemble $X$ the information measured in bits of an event $S\subset \mathcal A_X$ is given by:
\begin{equation}
h(\mathcal S) = -\log p_X(\mathcal S)
\end{equation}
\end{definition}
\begin{exercise}
Let $X$ be an ensemble modelling a fair coin, that is with alphabet $\mathcal A_X=\{\text{heads},\text{tails}\}$ and with $p_X$ the uniform distribution. What is the information of the event heads and of the event tails?
\end{exercise}
\begin{solution}
As $p_X$ is uniform, we have that $p(\text{heads})=p(\text{tails})=1/2$. Hence:
\begin{equation*}
h(\text{heads})=-\log(1/2)=1\text{ bit}
\end{equation*}
and
\begin{equation*}
h(\text{heads})=-\log(1/2)=1\text{ bit}\ .
\end{equation*}
\end{solution}
Let us end this section by checking that all our desired conditions hold. First since the $\log$ function is continuous and monotonically increasing in the range $(0,1]$ it holds that $h$ is also continuous and monotonically decreasing in the range. Finally, if two events $a,b$ are independent, $p(a \text{ and } b)=p(a)p(b)$ and in consequence 
\begin{align}
h(a \text{ and } b) &= -\log \left(p(a \text{ and } b)\right)\\
                    &= -\log \left(p(a) p(b)\right)\\
                    &= -\log(p(a)) -\log(p(b))\\
                    &= h(a)+h(b)
\end{align}
\booksection{Entropy}
\label{subsec:entropy}
We define the entropy of an ensemble as the average information content it provides:
\begin{definition} Let $X$ be an ensemble, the entropy of the ensemble is defined as: 
        \begin{equation}\label{eq:entr}
        H(\mathbf{X})=-\sum_{x} p(x)\log p(x)   
        \end{equation}
\noindent where we take the convention that $0\log 0=0$, i.e. adding a  zero-probability event to a source does not affect its entropy.
\end{definition}
We can rewrite the definition of entropy as the expectation of the random variable $h(X)$. That is a random variable that associated each event with the negative logarithm of its probability: 

\begin{equation}
  \label{eq:mean}
  H(\mathbf{X})=-\sum_{x} p(x)\log p(x) = E(-\log p(\mathbf{X}))
\end{equation}

Note that entropy only depends on the values of the probabilities. In the following we will sometimes be interested in the entropy a probability distribution independently of an ensemble. We will use the notation $H(p_1,\ldots,p_n)$ to indicate the probability distribution. Let us now investigate some basic properties of entropy that we will use through this course.

\begin{exercise}Show that entropy can not be negative.
  \label{lem:entropynonnegative}
  \begin{equation*}
    H(\mathbf{X})\geq 0
  \end{equation*}
\end{exercise}
\begin{solution}
  \begin{equation}
    0 \leq p(x) \leq 1 \Rightarrow -\log p(x) \geq 0 \Rightarrow H(X) \geq 0
  \end{equation}
\end{solution}

%\begin{definition} A function $f(x)$ defined on the interval $(a,b)$ is concave if any two points $x_1$, $x_2$ in the interval and any $t\in[0,1]$ verify:
%        \begin{equation}
%        f(tx_1+(1-t)x_2) \geq tf(x_1)+(1-t)f(x_2)
%         \end{equation}
%\end{definition}

%We prove the corollary of Jensen's inequality for concave functions as it demonstrates the non-negativity of the entropy function~\cite{Cover_91}.

The following is known as Jensen's inequality and will be of use in the following. See~\cite{Cover_91} for a proof.
\begin{theorem}[Jensen's inequality] Let $f$ be a concave function and $X$ a random variable. Then: 
  \label{th:jensen}
  \begin{equation*}
    f(E(\mathbf{X}))\geq E(f(\mathbf{X}))
  \end{equation*}
\end{theorem}
%
%\begin{proof}
%  We proceed to show by induction on the number of elements  in $\mathcal{X}$.
%  \begin{equation}
%    \label{eq:jensenbasis}
%    f(p_1x_1+p_2x_2) \geq p_1f(x_1)+p_2f(x_2)
%  \end{equation}
%  \noindent Eq.~\ref{eq:jensenbasis}, that follows from the concavity of $f(x)$, shows that Jensen's inequality holds for $|\mathcal{X}|=2$, if we assume that it holds for $|\mathcal{X}|=n$:
%
%  \begin{equation}
%    f(\sum_{i=1}^np_ix_i) \geq \sum_{i=1}^np_if(x_i)
%  \end{equation}
%  \noindent the induction hypothesis implies that it should also hold for $|\mathcal{X}|=n+1$
%
%  \begin{eqnarray}
%    f(\sum_{i=1}^{n+1}p_ix_i) 
%        & =    & f\left( p_{n+1}x_{n+1} + (1-p_{n+1}) \sum_{i=1}^{n}\frac{p_i}{1-p_{n+1}}x_i \right) \nonumber \\
%        &\geq & p_{n+1}f(x_{n+1}) + (1-p_{n+1}) f\left( \sum_{i=1}^{n}\frac{p_i}{1-p_{n+1}}x_i \right) \nonumber \\
%        &\geq & p_{n+1}f(x_{n+1}) + (1-p_{n+1}) \sum_{i=1}^{n}\frac{p_i}{1-p_{n+1}}f(x_i) \nonumber \\
%        & = &    p_{n+1}f(x_{n+1}) + \sum_{i=1}^{n}p_if(x_i) \nonumber \\
%        & = &   \sum_{i=1}^{n+1}p_if(x_i)
%  \end{eqnarray}
%  \noindent where the first inequality follows from the concavity of $f(x)$ and the second from the induction hypothesis.
%\end{proof}
%
%\begin{lemma}The entropy of the uniform distribution equals the logarithm of the number of elements.
%  \begin{equation*}
%    H(\dfrac{1}{n}, ..., \dfrac{1}{n}) = \log n
%  \end{equation*}
%\end{lemma}
%\begin{proof}
%  \begin{equation}
%    H(\dfrac{1}{n}, ..., \dfrac{1}{n}) = -\sum_{i=1}^n \dfrac{1}{n} \log \dfrac{1}{n} = \log n        
%  \end{equation}
%\end{proof}

\begin{lemma}
\label{lem:uniform}
The distribution that maximizes entropy for any alphabet is the uniform distribution.
\begin{equation*}
H(p_1, ..., p_n)\leq \log n
\end{equation*}
\end{lemma}
\begin{proof}
\begin{eqnarray}
H(p_1, ..., p_n)  - \log n & =      & \sum_{i=1}^n p_i \log \dfrac{1}{p_i} - \sum_{i=1}^n \dfrac{1}{n} \log n \nonumber \\
                                       & =      & \sum_{i=1}^n p_i \log \dfrac{1}{p_i} - \log n \sum_{i=1}^n \dfrac{1}{n} \nonumber \\
                                       & =      & \sum_{i=1}^n p_i \log \dfrac{1}{p_i} - \log n \sum_{i=1}^n p_i \nonumber \\
                                       & =      & \sum_{i=1}^n p_i \log \dfrac{1}{p_i} - \sum_{i=1}^n p_i \log n \nonumber \\
                                       & =      & \sum_{i=1}^n p_i \log \dfrac{1}{np_i} \nonumber \\                                      
                                       & \leq &  \log \sum_{i=1}^n \dfrac{1}{n} = 0\nonumber \\
\end{eqnarray}
\noindent where the second equality follows from the fact that a probability distribution adds up to one and the last inequality holds from $\log$ being a concave function and applying Jensen's inequality.
\end{proof}

%\subsection{Data Compression}
%Once presented an information measure, we review some of its operational interpretations, in particular its relation with the theoretical limits for data compression.
%
%A source code $C$ in alphabet $\mathcal{Y}$ for a random variable taking values in $\mathcal{X}$ is a function $c:\mathcal{X}\longrightarrow \mathcal{Y}^*$, where $\mathcal{Y}^*$ is any finite direct product of $\mathcal{Y}$. We call $c(x)$ the codeword for $x$ and $l(x)=|c(x)|$ the length of the codeword for $x$.
%
%The length of a source code $C$ is the sum of the lengths of every codeword in $C$ weighted by its relative frequency.
%\begin{equation}
%L(C(\mathbf X)) = \sum_{x} p(x) l(x)
%\end{equation}
%
%A source code is uniquely decodable if any concatenation of codewords can only be generated by a unique concatenation of source symbols. In other words, the source symbols generating the codewords can be recovered with no possible equivocation. 
%
%The next inequality on the length of uniquely decodable source codes was first proved by McMillan~\cite{McMillan_56} however the following proof is a simpler version by Karush~\cite{Karush_61,Cover_91}.
%
%\begin{theorem} The length of a uniquely decodable source code $C$ for a random variable $\mathbf{X}$ taking values in alphabet $\mathcal{Y}$ verifies:
%\begin{equation*}
%\sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \leq 1
%\end{equation*}
%\label{th:mcmillan}
%\end{theorem}
%\begin{proof}
%Let $c(x_1, x_2, ..., x_k)$ be a concatenation of codewords of aggregated length $l(x_1, x_2, ..., x_k)=\sum_{i=1}^k l(x_i)$. Since $C$ is uniquely decodable for any aggregated length $k$, no more than $|\mathcal{Y}|^k$ different concatenation of codewords can be generated. 
%
%We can consider the related expression on the aggregated length:
%\begin{eqnarray}
%\left( \sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{x_1}\frac{1}{|\mathcal{Y}|^{l(x_1)}} \sum_{x_2}\frac{1}{|\mathcal{Y}|^{l(x_2)}} \dots \sum_{x_n }\frac{1}{|\mathcal{Y}|^{l(x_n)}} \nonumber \\
%&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1)+l(x_2)+\dots +l(x_n)}} \nonumber \\
%&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1+x_2+\dots +x_n)}}
%\end{eqnarray}
%\noindent which can also be written as the sum for all possible lengths $i$ of the number $T_i$ of concatenation of $n$ codewords:
%
%\begin{eqnarray}
%\left( \sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{i=1}^{nl_{max}}\frac{T_i}{|\mathcal{Y}|^i} \nonumber \\
%&\leq & \sum_{i=1}^{nl_{max}}\frac{|\mathcal{Y}|^i}{|\mathcal{Y}|^i} \nonumber \\
%&\leq & nl_{max}
%\end{eqnarray}
%\noindent where $l_{max} = \max_{x}l(x)$. And taking the $n$-th root in both sides:
%
%\begin{eqnarray}
%\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq & (nl_{max})^{1/n}
%\end{eqnarray}
%\noindent now since the limit $\lim_{n \to \infty } (nl_{max})^{1/n} = 1$ and the result holds for all $n$:
%
%\begin{eqnarray}
%\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq 1
%\end{eqnarray}
%\end{proof}
%
%We finish this brief overview of source coding with Th.~\ref{th:sourcecoding}~\cite{Cover_91}, it shows that the length  of a uniquely decodable code is lower bounded by the entropy of the random variable.
%\begin{theorem}
%\label{th:sourcecoding}
%The length of a uniquely decodable code taking values from finite alphabet $\mathcal{Y}$ for random variable $\mathbf{X}$ is lower bounded by the entropy of $\mathbf{X}$.
%\begin{equation*}
%L\geq H_{|\mathcal{Y}|}(\mathbf{X})
%\end{equation*}
%\begin{proof}
%\begin{eqnarray}
%H_{|\mathcal{Y}|}(\mathbf{X}) - L &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x}p(x)l(x) \nonumber \\
%                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x} p(x)  \log_{|\mathcal{Y}|} |\mathcal{Y}|^{-l(x)} \nonumber \\
%                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{|\mathcal{Y}|^{-l(x)} }{p(x)} \nonumber \\
%                                      &\leq & \log_{|\mathcal{Y}|} \sum_{x} |\mathcal{Y}|^{-l(x)} \nonumber \\
%                                      &\leq & \log_{|\mathcal{Y}|} 1 = 0
%\end{eqnarray}
%\noindent where the first inequality is again an application of Jensen's result Th.~\ref{th:jensen} and the second one results from applying McMillan's Th.~\ref{th:mcmillan}.
%\end{proof}
%\end{theorem}
%
%This result can be extended to consider a source code $C$ for a sequence of $n$ random variables {iid} from the ensemble $\mathbf{X}$. That is, a source  $\mathbf{X}$ as we defined it in Sec.~\ref{subsec:entropy}. Let $R=L(C(\mathbf {X^n}))/n$, the length per symbol of $C$, be the encoding rate of the source $\mathbf{X}$. It is trivial to see that the rate is also lower bounded by the entropy of the source:
%
%\begin{eqnarray}
%R &=& L(C(\mathbf {X^n}))/n \nonumber\\
%  &\geq& H_{|\mathcal{Y}|}(\mathbf{X^n})/n \nonumber\\
% &=& H_{|\mathcal{Y}|}(\mathbf{X})
%\end{eqnarray}

\booksection{Joint entropy, conditional entropy and mutual information}
We will now explore three information measures that derive from entropy as we defined it in the previous section. The first measure is joint entropy, which is a direct application of the definition of entropy to a joint source.
\begin{definition}
Given two ensembles $\mathbf{X}$ and $\mathbf{Y}$ the entropy of the joint ensemble $XY$ is given by:
\begin{equation}
H(\mathbf{XY}) = - \sum_{x,y} p(x,y)\log p(x,y)
\end{equation}
\end{definition}

Exercise \ref{ex:infcontext} suggests that the information content depends on the context. The second information measure that we introduce is conditional entropy.
First, we can extend in a straightforward way the reasoning in Sec.~\ref{subsec:information} to define an information measure conditional on the knowledge of some event $y$. 
It can analogously be proved that a conditional information measure is of the form: 
\begin{equation}
h(a|b) = -\log p (a|b)
\end{equation}
Let $XY$ be a joint ensemble, we can define the conditional entropy of $X$ given the event $y$ as the average conditional information:
\begin{equation}
H(\mathbf{X}|y) = \sum_x p(x|y) h(x|y)
\end{equation}
and the conditional entropy of $X$ given ensemble $Y$:
\begin{equation}
H(\mathbf{X}|\mathbf{Y})=\sum_yH(X|y)
\end{equation}
\begin{exercise}
Show that $H(X|Y)=H(XY)-H(Y)$.
\end{exercise}
Let us investigate some basic properties of the conditional entropy.
\begin{exercise}Show that the conditional entropy is non-negative.
\begin{equation*}
H(\mathbf{X}|\mathbf{Y}) \geq 0
\end{equation*}
\end{exercise}
\begin{solution}
$H(\mathbf{X}|\mathbf{Y})$ is a sum of entropies, which are positive by Lem.~\ref{lem:entropynonnegative}, weighed by the probabilities of each event which are also positive.
\end{solution}

\begin{exercise}
\label{lem:entgeqcond}
Show that the entropy of the random variable $\mathbf{X}$ given any random variable $\mathbf{Y}$ is not greater than the entropy of $\mathbf{X}$.
\begin{equation*}
H(\mathbf{X}|\mathbf{Y}) \leq H(\mathbf{X})
\end{equation*}
\end{exercise}
\begin{solution}
\begin{eqnarray}
H(\mathbf{X}|\mathbf{Y}) - H(\mathbf{X}) &=& \sum_{y }p(y) \sum_{x} p(x|y)\log \frac{1}{p(x|y)} - \sum_{x}p(x) \log  \frac{1}{p(x)}\nonumber \\
                         &=& \sum_{y}\sum_{x}p(x,y) \log \frac{1}{p(x|y)} + \sum_{x,y}p(x,y) \log  p(x) \nonumber \\
                         &=& \sum_{x,y}p(x,y) \log  \frac{p(x)}{p(x|y)} \nonumber \\
                         &=& \sum_{x,y}p(x,y) \log  \frac{p(x)p(y)}{p(x,y)} \nonumber \\
                         &\leq & \log \sum_{x,y} p(x) p(y) = 0
\end{eqnarray}
\end{solution}

\begin{exercise}
\label{lem:function}
Given random variables $\mathbf{X}$ and $\mathbf{Y}$ if $\mathbf{X}=f(\mathbf{Y})$: 
\begin{equation*}
H(\mathbf{X}|\mathbf{Y}) = 0
\end{equation*}
\end{exercise}
\begin{solution}
If $\mathbf{X}=f(\mathbf{Y})$, then given $\mathbf{Y}$ we know $\mathbf{X}$ with absolute certainty, in other words, given $\mathbf{Y}$ there is just one possible outcome.
\begin{eqnarray}
H(\mathbf{X}|\mathbf{Y})&=&\sum_{y}p(y) H(\mathbf{X}|y)\nonumber\\
         &=& 0
\end{eqnarray}
\end{solution}

%We now derive the so called chain rule of joint entropy.

\begin{exercise} Show that the following relation holds for any two ensembles $XY$:
\begin{equation*}
H(\mathbf{XY}) = H(\mathbf{X}) + H(\mathbf{Y}|\mathbf{X})
\end{equation*}
\end{exercise}
\begin{solution}
\begin{eqnarray}
H(\mathbf{XY}) &=& - \sum_{x,y} p(x,y)\log p(x,y) \nonumber \\
       &=& - \sum_{x}p(x)\sum_{y} p(y|x)\log p(x)p(y|x) \nonumber \\
       &=& - \sum_{x}p(x)\log p(x)\sum_{y} p(y|x) \nonumber \\
       & & - \sum_{x}p(x)\sum_{y} p(y|x)\log p(y|x) \nonumber \\
       &=& H(\mathbf{X}) + H(\mathbf{Y}|\mathbf{X})
\end{eqnarray}
\end{solution}

%The joint and conditional entropy definitions can be also naturally extended to multiple variables. %We show a %second 
%chain rule for the joint entropy of $\mathbf{X}$ and $\mathbf{Y}$ given a variable $\mathbf{Z}$.  
%\begin{lemma}
%\label{lem:chain2}
%Let $\mathbf{X}$, $\mathbf{Y}$, and $\mathbf{Z}$ be three random variables, then:
%\begin{equation*}
%H(\mathbf{XY}|\mathbf{Z}) = H(\mathbf{X}|\mathbf{Z}) + H(\mathbf{Y}|\mathbf{X}\mathbf{Z})
%\end{equation*}
%\end{lemma}
%\begin{proof}
%\begin{eqnarray}
%H(\mathbf{XY}|\mathbf{Z}) &=& - \sum_{z}p(z)H(\mathbf{X}\mathbf{Y}|z)\nonumber \\
%         &=& - \sum_{z}p(z) \sum_{x,y} p(x,y|z)\log p(x,y|z) \nonumber \\
%         &=& - \sum_{z}p(z) \sum_{x} p(x|z)\sum_{y} p(y|xz)\log p(x|z)p(y|xz) \nonumber \\
%         &=& - \sum_{z}p(z) \sum_{x} p(x|z)\log p(x|z)\sum_{y} p(y|xz) \nonumber \\
%         & & - \sum_{z}p(z) \sum_{x}p(x|z)\sum_{y} p(y|xz)\log p(y|xz)  \nonumber \\ 
%         &=& H(\mathbf{X}|\mathbf{Z}) + H(\mathbf{Y}|\mathbf{X}\mathbf{Z})  
%\end{eqnarray}
%\end{proof}

The third information measure that we introduce is the mutual information:
\begin{definition}
Given a joint ensemble $XY$, we define the mutual information between $X$ and $Y$ by:
\begin{equation*}
I(X;Y)=H(X) + H(Y) - H(XY)
\end{equation*}
\end{definition}
The mutual information $I(\mathbf{X};\mathbf{Y})$ is a measure of the information shared between the two variables $\mathbf{X}$ and $\mathbf{Y}$. Let us make this intuition more precise:
\begin{exercise}
Show that for any ensemble $X$: $I(X;X)=H(X)$.
\end{exercise}
\begin{exercise}
Show that $I(X;Y)=0$ if and only if $X$ and $Y$ are independent. 
\end{exercise}
\begin{exercise}
Show that $I(X;Y)\geq 0$
\end{exercise}

Fig.~\ref{fig:infmeasures} shows the relationship between the four measures that we have defined: entropy, joint entropy, conditional entropy and mutual information.

\begin{figure}
\begin{center}
\def\svgwidth{.8\columnwidth} 
\input{figures/infmeasures.pdf_tex} 
\caption{Graphical representation of the information measures.}
\label{fig:infmeasures}
\end{center}
\end{figure}

\begin{eqnarray}
\label{eq:mutualinformation}
I(\mathbf{X};\mathbf{Y}) &=& H(\mathbf{Y}) - H(\mathbf{Y}|\mathbf{X}) \nonumber\\
         &=& H(\mathbf{X}) - H(\mathbf{X}|\mathbf{Y}) \nonumber\\
         &=& I(\mathbf{Y};\mathbf{X})
\end{eqnarray}
\booksection{Exercises}
\begin{exercise}
Let $X$ be a random variable with $H(X)>0$ and let $Y=f(X)$. 
\begin{enumerate}
\item Give one function such that $H(Y)=H(X)$
\item Give one function such that $0<H(Y)<H(X)$
\item Give one function such that $H(Y)=0$
\end{enumerate}
\end{exercise}
\begin{exercise}
A classic logical problem (also classic in information theory texts!) states that you receive 12 coins one of which is a counterfeit. The counterfeit is either lighter or heavier than the normal coins, you do not know which is the case. Fortunately, you have access to a two-plate scale that can compare weights.
\begin{enumerate}
\item Give a non-trivial bound on the minimum number of weighings that could give the answer.
\item Give an strategy that solves the problem if you know tha the counterfeit coin is heavier than normal coins.
\item Give an strategy that solves the general problem.
\end{enumerate}
\end{exercise}
\begin{exercise}
Convex mixtures
\end{exercise}
\begin{exercise}
Entropy of a sum goes to the minitest
\end{exercise}
\begin{exercise}
World series like exercise
\end{exercise}
\begin{exercise}
to minitet, for $X$, $Y$ independent $H(X,Y)=H(X)+H(Y)$.
\end{exercise}
\booksection{Further reading}
The mathematical foundations of information theory were to a certain extent developped single handedly by Claude Shannon. His original paper \cite{Shannon_48} developped the framework and also solved some of the most important problems. The text has not aged with time and remains a greatly written and accessible introduction to the field. A second excellent source for digging deeper into the material is the book of Cover and Thomas \cite{Cover_91}, it is the reference of the field and widely used in most introductory courses on information theory. 

In section \ref{sec:axent} we sketched an axiomatic derivation of entropy. For a complete discussion on axiomatic derivations of entropy and information please refer to~ \cite{Aczel_74,Aczel_75,Csiszar_08,Feinstein_58}. 
