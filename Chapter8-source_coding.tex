\chapterimage{HRP6KJ.jpg} 
\chapter{Source coding}
%------------------------------------------------
In the previous chapter we posed in an abstract way a series of conditions that information measures should possess. We built on top of those conditions and found a series of information measures satisfying them. Moreover, in a certain sense those measures are unique.

In this chapter we will begin a journey to show that not only entropy is a good measure for information, but also that it carries a strong operational meaning. In fact, we will show that matching  our intuition, if a source has a certain entropy, i.e. it contains a certain amount of information. Then, then the length of the message that we need to send to some other party in order to transmit the source needs to be at least the entropy of the source. 

We will also begin to investigate the difference between proofs of existance and constructive methods. This will be a common pattern in information theory, though of limited importance in our introductory course. In fact, it is in many circumstances, possible to prove that codes with desired properties must exist by relying on random ensembles of codes, while actually pointing an example is more complicated.

\booksection{Transmitting the weather forecast}
Let us first go back to our example from the previous chapter. It will give us enough material to formalize the discussion. Let us suppose again that we have a distant but commited friend that has agreed to send us a daily message summarizing the weather forecast. However, we want a little bit more information that in the previous chapter and now we want our friend to tell us whether tomorrow will be sunny, will snow, will rain or will GRANIZAR. Let us investigate different ways in which our friend can communicate this information to us:

A first simple way would be to send a message containing the appropriate word: "sun", "rain", "snow", "GRAN". However, since there are only four possible messages, we could instead just send a number: "0" for rain, "1" for sun, 2 for snow and 3 for GRAN. 

\begin{definition}
Code
\end{definition}
We could be more sophisticated. Let us assume that the four events occur with the following probability: it rains with probability $1/2$, it snows with probability $1/4$, and both sunny days and GRAn days happen with probability $1/8$. We could assign the following words to each event: rain $0$, snow $10$, sun $110$ and GRAN $111$. That is, we have assigned more likely events shorter words? Why is this better?

\begin{definition}
Given a discrete random variable $X$ taking events in the finite alphabet $\mathcal X$ and a code $C$ from $\mathcal X$ to the code alphabet $\mathcal X$, we define the average length of the code as follows:
\begin{equation}
l(C)=\sum_{x\in\mathcal X}p(x)|C(x)|
\end{equation}
where $|C(x)|$ is the length of the codeword of event $x$.
\end{definition}
\begin{exercise}
With the definition of average legnth, we find that the first code taking each event to a two bit word, has average length 2 while the second code with variable length has average length 1.375 which is significantly shorter. 
\end{exercise}

Can we do better than 1.375? We can indeed, consider the code that assigns the events rain and sun the word 0 and the events snow and GRAN the word 1. 
This code has average length 1. 
However, what is the problem? 
The problem is that if we receive the message 0, we can not know whether tomorrow is going to rain or to be sunny. 
The code is not-singular.

\begin{definition}
A code $C:\mathcal X\mapsto\mathcal Y$ is singular if $\forall x,y\in\mathcal X$ with $x\neq y$ $C(x)\neq C(y)$.
\end{definition}

However, in order to recover the information content, it is not enough for a code to be singular. Consider the following code: rain 0, sun 1, gran $01$ and snow $10$. 
And suppose that our friend wants to send the forecast of two consecutive days. 
If we receive the message $010$, we could decode it as $01$ and $0$ with the meaning first day gran second day rain or we could decode it as $0$ and $10$ with the meaning rain and snow.
The code can not be uniquely decoded.

\begin{definition}
Uniquely decodable
\end{definition}

In the example, it was fairly easy to spot that the code was not uniquely decodable. However, for some codes it is not so simple.
\begin{exercise}
Let $C=\{asdfsadf\}$ is $C$ uniquely decodable? (Hint: if you can not find the solution, continue reading and return to this exercise once you have understood the Sardinas-Patterson method)
\end{exercise}

Let us now investigate a method to find if a code is uniquely decodable or not. This method was proposed by Sardinas and Patterson \cite{} and is also known as the method of the dangling suffixes. 


\booksection{Formal problem}
\subsection{Data Compression}
Once presented an information measure, we review some of its operational interpretations, in particular its relation with the theoretical limits for data compression.

A source code $C$ in alphabet $\mathcal{Y}$ for a random variable taking values in $\mathcal{X}$ is a function $c:\mathcal{X}\longrightarrow \mathcal{Y}^*$, where $\mathcal{Y}^*$ is any finite direct product of $\mathcal{Y}$. We call $c(x)$ the codeword for $x$ and $l(x)=|c(x)|$ the length of the codeword for $x$.

The length of a source code $C$ is the sum of the lengths of every codeword in $C$ weighted by its relative frequency.
\begin{equation}
L(C(\mathbf X)) = \sum_{x} p(x) l(x)
\end{equation}

A source code is uniquely decodable if any concatenation of codewords can only be generated by a unique concatenation of source symbols. In other words, the source symbols generating the codewords can be recovered with no possible equivocation. 

The next inequality on the length of uniquely decodable source codes was first proved by McMillan~\cite{McMillan_56} however the following proof is a simpler version by Karush~\cite{Karush_61,Cover_91}.

\begin{theorem} The length of a uniquely decodable source code $C$ for a random variable $\mathbf{X}$ taking values in alphabet $\mathcal{Y}$ verifies:
\begin{equation*}
\sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \leq 1
\end{equation*}
\label{th:mcmillan}
\end{theorem}
\begin{proof}
Let $c(x_1, x_2, ..., x_k)$ be a concatenation of codewords of aggregated length $l(x_1, x_2, ..., x_k)=\sum_{i=1}^k l(x_i)$. Since $C$ is uniquely decodable for any aggregated length $k$, no more than $|\mathcal{Y}|^k$ different concatenation of codewords can be generated. 

We can consider the related expression on the aggregated length:
\begin{eqnarray}
\left( \sum_{x}\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{x_1}\frac{1}{|\mathcal{Y}|^{l(x_1)}} \sum_{x_2}\frac{1}{|\mathcal{Y}|^{l(x_2)}} \dots \sum_{x_n }\frac{1}{|\mathcal{Y}|^{l(x_n)}} \nonumber \\
&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1)+l(x_2)+\dots +l(x_n)}} \nonumber \\
&=& \sum_{x_1,x_2,\dots,x_n }\frac{1}{|\mathcal{Y}|^{l(x_1+x_2+\dots +x_n)}}
\end{eqnarray}
\noindent which can also be written as the sum for all possible lengths $i$ of the number $T_i$ of concatenation of $n$ codewords:

\begin{eqnarray}
\left( \sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}} \right)^n &=& \sum_{i=1}^{nl_{max}}\frac{T_i}{|\mathcal{Y}|^i} \nonumber \\
&\leq & \sum_{i=1}^{nl_{max}}\frac{|\mathcal{Y}|^i}{|\mathcal{Y}|^i} \nonumber \\
&\leq & nl_{max}
\end{eqnarray}
\noindent where $l_{max} = \max_{x}l(x)$. And taking the $n$-th root in both sides:

\begin{eqnarray}
\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq & (nl_{max})^{1/n}
\end{eqnarray}
\noindent now since the limit $\lim_{n \to \infty } (nl_{max})^{1/n} = 1$ and the result holds for all $n$:

\begin{eqnarray}
\sum_{x }\frac{1}{|\mathcal{Y}|^{l(x)}}  &\leq 1
\end{eqnarray}
\end{proof}

We finish this brief overview of source coding with Th.~\ref{th:sourcecoding}~\cite{Cover_91}, it shows that the length  of a uniquely decodable code is lower bounded by the entropy of the random variable.
\begin{theorem}
\label{th:sourcecoding}
The length of a uniquely decodable code taking values from finite alphabet $\mathcal{Y}$ for random variable $\mathbf{X}$ is lower bounded by the entropy of $\mathbf{X}$.
\begin{equation*}
L\geq H_{|\mathcal{Y}|}(\mathbf{X})
\end{equation*}
\end{theorem}
\begin{proof}
\begin{eqnarray}
H_{|\mathcal{Y}|}(\mathbf{X}) - L &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x}p(x)l(x) \nonumber \\
                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{1}{p(x)} - \sum_{x} p(x)  \log_{|\mathcal{Y}|} |\mathcal{Y}|^{-l(x)} \nonumber \\
                                      &=& \sum_{x} p(x) \log_{|\mathcal{Y}|} \frac{|\mathcal{Y}|^{-l(x)} }{p(x)} \nonumber \\
                                      &\leq & \log_{|\mathcal{Y}|} \sum_{x} |\mathcal{Y}|^{-l(x)} \nonumber \\
                                      &\leq & \log_{|\mathcal{Y}|} 1 = 0
\end{eqnarray}
\noindent where the first inequality is again an application of Jensen's result Th.~\ref{th:jensen} and the second one results from applying McMillan's Th.~\ref{th:mcmillan}.
\end{proof}

This result can be extended to consider a source code $C$ for a sequence of $n$ random variables {iid} from the ensemble $\mathbf{X}$. That is, a source  $\mathbf{X}$ as we defined it in Sec.~\ref{subsec:entropy}. Let $R=L(C(\mathbf {X^n}))/n$, the length per symbol of $C$, be the encoding rate of the source $\mathbf{X}$. It is trivial to see that the rate is also lower bounded by the entropy of the source:

\begin{eqnarray}
R &=& L(C(\mathbf {X^n}))/n \nonumber\\
  &\geq& H_{|\mathcal{Y}|}(\mathbf{X^n})/n \nonumber\\
 &=& H_{|\mathcal{Y}|}(\mathbf{X})
\end{eqnarray}

\booksection{Huffman codes}
\booksection{Typical sequences}
\booksection{Shannon's theorem for source coding}
\booksection{Further reading}
The references of the previous chapter are also relevant for this one. In particular sections X and Y of Shanon's original paper \cite{} and chapters A and B in Cover and Thomas \cite{}. A popular introduction to source coding is provided by xkcd (the comic stripe) \cite{}, in the post you will recognize the image that opens the chapter, perhaps you can explain it's relation with the topic. %https://what-if.xkcd.com/34/
