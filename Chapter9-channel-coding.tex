\chapterimage{New_horizons_(NASA).jpg} 
\chapter{Channel coding}
In the previous chapter we investigated the source coding problem. That is, given a source and some noiseless means of communication, the problem is to encode the source in such a way that we minimize the usage of the noiseless communications channel while we allow the receiver to recover the message. Here we will investigate a dual problem, the problem of transmitting a uniform source over a noisy channel. This problem, is the problem that your mobile phone faces each time that it wants to exchange information with the nearest base station, or the problem that your ADSL router faces to transmit information over fiber. It is also the same problem that your computer faces when it wants to store information on a disk in such a way that it can be recovered at a later time. This is as we see a very relevant problem. We won't have the time to dig into great depth, but the menu should allow you to get a solid understanding of the mathematical foundations and a glimpse about how we tackle it currently. In the figure above, you can see the XXXX space XXX. As you can imagine, the bandwidth of the communications channel linking it with earth is very limited, the information we want to exchange precious and the channel rather noisy. Space exploration has been one unexpected driver of progress for pushing the limits of error correcting codes. (why error correting?)
\booksection{The communications problem}
\begin{figure}[h!]
\begin{center}
\def\svgwidth{\columnwidth} 
\input{gfx/shannoncom.pdf_tex}
\end{center}
\caption[Communications system diagram.]{This figure reproduces the communications system diagram introduced by Shannon~\cite{Shannon_48}.}
\label{fig:shannoncom}
\end{figure}
Let us first of all, depict the building blocks of an idealized communications problem. Our description parallels the one of Shannon~\cite{Shannon_48}, see in Fig.~\ref{fig:shannoncom} a graphical representation. The figure shows five entities: an information source, a transmitter, a noise source, a receiver, and a destination. The communications scheme works as follows: 

First the information source generates a message $\mathbf m$ from a set of possible messages $M$. Then, the transmitter takes $\mathbf m$ and encodes it into $n$ channel symbols. We define the coding rate $R$ as:

\begin{equation}
R=\frac{\log M}{n}
\end{equation}

\begin{exercise}
Suppose that we want to transmit a message from a set of $16$ messages that a source will choose uniformly at random. We have two channels available, both are noiseless, one allows to transmit one bit per channel use and we can use it once per second. The second channel allows to transmit two bits per channel use and we can use it once per minute. Over which channel can we communicate at a higher rate? How does time influence this number?
\end{exercise}
The channel is a physical medium of transmission. Mathematically, we can model it as a system taking symbols from input alphabet $\mathcal{X}$ to symbols of output alphabet $\mathcal{Y}$ and characterized by a transition probability matrix that maps the probability of every symbol $y$ if symbol $x$ is sent. The receiver tries to undo the encoding given the noisy received signal and at the end of the scheme the destination receives the $\mathbf{\hat{m}}$ possibly identical to $\mathbf m$.

We define $C$, the capacity of a channel, as the maximum mutual information for all possible input distributions: 

\begin{eqnarray}
\label{eq:capform}
C = \max_{p(x)} I(\mathbf X;\mathbf Y)
\end{eqnarray}

\booksection{Linear codes}

\booksection{Hamming codes}

\booksection{Detection, correction and minimum distance}
\booksection{Random codes}
\booksection{Shannon's second theorem}

The capacity of a channel specifies the maximum rate at which a source can be reliably sent through a channel. On the other hand, no source with a rate over the capacity of the channel can be sent with a vanishing error probability.

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth} 
\input{gfx/channel.pdf_tex} 
\caption[Jointly typical sequences]{Graphical representation of the input and output typical sequences. A good encoding chooses as codewords a subset of the input typical sequences that produces disjoint sets of output typical sequences.}
\label{fig:channel}
\end{center}
\end{figure}

A sketch of the proof would be as follows. Encoder and decoder share a code-book of $2^{nR}$ codewords chosen within the $2^{nH(\mathbf X)}$ typical sequences~\cite{Massey_77}. The encoder sends a codeword $\mathbf{x}$ drawn with uniform probability. The decoder outputs a word $\hat{\mathbf{x}}$ jointly typical with the received word $\mathbf{y}$. It declares an error if $\mathbf{x}$, $\mathbf{y}$ are not jointly typical and a decoding error can occur if there exists $\mathbf{x}'\neq \mathbf{x}$ jointly typical with $\mathbf{y}$. We know by Eq.~\ref{eq:jointtypical} that the probability of non-joint typicality for long enough $n$ can be made as small as desired. %We can bound the second source of error $p_e$ as well:

%\begin{eqnarray}
%p_e &\leq&  \left( 2^{nR}-1 \right) \frac{2^{n(H(\mathbf{XY})-\delta )}}{2^{n(H(\mathbf X)-\delta )}2^{n(H(\mathbf Y)-\delta )}}\nonumber\\
%     &\leq & 2^{-n(I(\mathbf X;\mathbf Y)-R-3\delta)}
%\end{eqnarray}
%\noindent in the first inequality we have bounded the error probability as the probability that one among the $2^{nR-1}$ remaining codewords belongs to the set of jointly typical sequences. The error probability vanishes only if $R<I(\mathbf X;\mathbf Y)$. However, we did not impose any condition on the $p(x)$ in the above discussion. In other words, if we choose the distribution that maximizes $I(\mathbf X;\mathbf Y)$, the typical sequence encoding achieves the capacity of the channel.

The intuition behind the achievability proof is simple. The decoder has access to two sets: the set of sequences jointly typical with $\mathbf{y}$, and the set of codewords. If the intersection is to be a single word, every codeword has to be jointly typical with a disjoint set of typical output words. 

Approximately, every codeword is jointly typical with $2^{nH(\mathbf Y|\mathbf X)}$ words. Then the number of jointly typical output words with input codewords is upper bounded by $2^{nR+nH(\mathbf Y|\mathbf X)}$, where $R$ is the coding rate. This number should be much smaller than the total number of typical sequences $2^{nH(\mathbf Y)}$:
 
\begin{equation*}
2^{nR+nH(\mathbf Y|\mathbf X)} < 2^{nH(\mathbf Y)}
\end{equation*}

\noindent which operating returns the expected result:

\begin{equation*}
R < I(\mathbf X;\mathbf Y)
\end{equation*}

In conclusion, as long as the coding rate is below the mutual information between input and output for $n$ long enough we can construct a code that allows the decoder to distinguish between codewords with a vanishing probability of error.

The converse statement follows from Fano's inequality \cite{Fano_61}. The intuition behind this part is that if we think of an encoding that achieves a vanishing error probability, then necessarily $R<I(\mathbf X;\mathbf Y)$~\cite{Cover_91}. 

\begin{theorem}
Fano's inequality
\end{theorem}
\begin{proof}
a
\end{proof}
\booksection{The capacity of some basic channels}

\subsection{Binary Symmetric Channel}
In the {BSC} the binary elements or bits are either perfectly transmitted with probability $1-p$ or flipped with probability $p$. 

Let us first find the mutual information between the input $\mathbf X$ and the output $\mathbf Y$~\cite{Cover_91}::

\begin{figure}[h]
\begin{center}
\def\svgwidth{\columnwidth} 
\input{gfx/bsc.pdf_tex} 
\caption{Binary Symmetric Channel.}
\label{fig:bsc}
\end{center}
\end{figure}
%\begin{proof}

\begin{eqnarray}
I(\mathbf X;\mathbf Y) &=& H(\mathbf Y) - H(\mathbf Y|\mathbf X) \\
         &=& H(Y) - \sum_x p(x)H(\mathbf Y|x) \\
         &=& H(Y) - \sum_x p(x)H(p,1-p) \\
         &=& H(Y) - H(p,1-p)\sum_x p(x) \\
         &\leq & 1 - H(p,1-p)\label{eq:bscC} 
\end{eqnarray}

We obtain the capacity by finding the maximum of the mutual information for all possible input distributions. It can be easily verified that the the uniform distribution reaches the upper bound in Eq.~\ref{eq:bscC} and the capacity of the {BSC} is one minus the binary entropy of $p$.
%and this capacity is achieved for $p(\mathbf X=0)=p(\mathbf X=1)=\frac{1}{2}$.
%\end{proof}

\subsection{Binary Erasure Channel}
The {BEC} was introduced by Elias in his famous paper "Coding for Two Noisy Channels"~\cite{Elias_55}. The {BEC} has two input elements while the output alphabet is composed of three elements: 0, 1, and $e$, which stands for an erasure in the channel. In this channel the bits are either correctly transmitted with probability $1-p$, or are erased with probability $p$. 

We can first find $H(\mathbf X|\mathbf Y) $: 

\begin{figure}[h]
\begin{center}
\def\svgwidth{\columnwidth} 
\input{gfx/bec.pdf_tex} 
\caption{Binary Erasure Channel.}
\label{fig:bec}
\end{center}
\end{figure}

%\begin{proof}

%\begin{eqnarray}
%\label{eq:beccap1}
%H(\mathbf Y) &=& H((1-p )(1-\pi ),p , (1-p ) \pi) \\
%        &=&  H(1-p , p ) + (1-p ) H(\pi, 1 - \pi )
%\end{eqnarray}
%\begin{eqnarray}
%\label{eq:beccap2}
%H(\mathbf Y|\mathbf X) &=& (1 -\pi )H(p, 1-p ) + \pi H(p, 1- p) \\
%           &=& H(p, 1- p)
%\end{eqnarray}
\begin{eqnarray}
\label{eq:beccap}
H(\mathbf X|\mathbf Y) &=& \pi (1-p )H(\mathbf X|\mathbf Y=0) \nonumber\\
           && + \left( \pi\, p +(1-\pi)p\right) H(\mathbf X|\mathbf Y=e)\nonumber\\
           && + (1-\pi)(1-p)H(\mathbf X|\mathbf Y=1) \\
           &=& p
\end{eqnarray}
\noindent where $p(\mathbf X=0)=\pi$.The second equality holds from $H(\mathbf X|\mathbf Y=1)=H(\mathbf X|\mathbf Y=0)=0$ and $H(\mathbf X|\mathbf Y=e)=1$. We can now plug Eq.~\ref{eq:beccap} in Eq.~\ref{eq:mutualinformation} and bound from above the mutual information:
\begin{eqnarray}
I(\mathbf X;\mathbf Y) &=& H(\mathbf X) - H(\mathbf X|\mathbf Y) \\
         &=& H(\pi, 1-\pi) - p\\
         & \leq & 1 - p\label{eq:becineq}
\end{eqnarray}
\noindent equality in Eq.~\ref{eq:becineq} is achieved again by the uniform distribution. That is, for $\pi=\frac{1}{2}$.
%\end{proof}

%\begin{eqnarray}
%C = \max_{p(x)} I(\mathbf X;\mathbf Y) = 
%\end{eqnarray}

It might seem that the capacity of a {BSC} that flips bits with probability $p$ is greater than the capacity of a {BEC} that erases bits with probability $p$. Fig.~\ref{fig:becbsc} shows that it is the opposite situation. On the range $p\in\left( 0,0.5\right)$, the capacity of the {BEC} is greater than the capacity of the {BSC}. Bits on the {BEC} are either perfectly known or perfectly unknown, however, it is not possible to distinguished flipped bits from correct bits in the {BSC}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{gfx/becbsc.pdf}
\caption{The capacity of the BEC and BSC.}
\label{fig:becbsc}
\end{center}
\end{figure}

\booksection{Further reading}
%------------------------------------------------
\booksection{Exam problems}

One time pad?
